{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "260a3450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "from gym import spaces\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n",
    "\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from typing import Callable, Dict, List, Tuple, Type, Union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3228898",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_features_path = 'data/movie_features.csv'\n",
    "rating_features_path = 'data/ratings_features.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f19e98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>tag</th>\n",
       "      <th>encoded_movie_ids</th>\n",
       "      <th>desc</th>\n",
       "      <th>desc_embedding_glove</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>comedy drama romance</td>\n",
       "      <td>['chick flick', 'revenge', 'characters', 'chic...</td>\n",
       "      <td>0</td>\n",
       "      <td>Waiting to Exhale (1995) | comedy, drama, roma...</td>\n",
       "      <td>[-3.90065998e-01  2.85064846e-01  1.03168570e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>American President, The (1995)</td>\n",
       "      <td>comedy drama romance</td>\n",
       "      <td>['president', 'president', 'us president', 'wh...</td>\n",
       "      <td>1</td>\n",
       "      <td>American President, The (1995) | comedy, drama...</td>\n",
       "      <td>[ 5.35968468e-02  2.33869895e-01  2.60694951e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>Casino (1995)</td>\n",
       "      <td>crime drama</td>\n",
       "      <td>['de niro in pink', 'joe pesci', 'martin scors...</td>\n",
       "      <td>2</td>\n",
       "      <td>Casino (1995) | crime, drama | de niro in pink...</td>\n",
       "      <td>[-0.09848612 -0.4099722  -0.08234926 -0.131738...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>Shanghai Triad (Yao a yao yao dao waipo qiao) ...</td>\n",
       "      <td>crime drama</td>\n",
       "      <td>['gong li', 'yimou zhang', 'zhang yimou']</td>\n",
       "      <td>3</td>\n",
       "      <td>Shanghai Triad (Yao a yao yao dao waipo qiao) ...</td>\n",
       "      <td>[ 0.09402635  0.07983646  0.02374954 -0.276408...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36</td>\n",
       "      <td>Dead Man Walking (1995)</td>\n",
       "      <td>crime drama</td>\n",
       "      <td>['nun', 'death row', 'capital punishment', 'co...</td>\n",
       "      <td>4</td>\n",
       "      <td>Dead Man Walking (1995) | crime, drama | nun, ...</td>\n",
       "      <td>[ 2.41189212e-01 -4.62112427e-02  1.89653516e-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                                              title  \\\n",
       "0        4                           Waiting to Exhale (1995)   \n",
       "1       11                     American President, The (1995)   \n",
       "2       16                                      Casino (1995)   \n",
       "3       30  Shanghai Triad (Yao a yao yao dao waipo qiao) ...   \n",
       "4       36                            Dead Man Walking (1995)   \n",
       "\n",
       "                 genres                                                tag  \\\n",
       "0  comedy drama romance  ['chick flick', 'revenge', 'characters', 'chic...   \n",
       "1  comedy drama romance  ['president', 'president', 'us president', 'wh...   \n",
       "2           crime drama  ['de niro in pink', 'joe pesci', 'martin scors...   \n",
       "3           crime drama          ['gong li', 'yimou zhang', 'zhang yimou']   \n",
       "4           crime drama  ['nun', 'death row', 'capital punishment', 'co...   \n",
       "\n",
       "   encoded_movie_ids                                               desc  \\\n",
       "0                  0  Waiting to Exhale (1995) | comedy, drama, roma...   \n",
       "1                  1  American President, The (1995) | comedy, drama...   \n",
       "2                  2  Casino (1995) | crime, drama | de niro in pink...   \n",
       "3                  3  Shanghai Triad (Yao a yao yao dao waipo qiao) ...   \n",
       "4                  4  Dead Man Walking (1995) | crime, drama | nun, ...   \n",
       "\n",
       "                                desc_embedding_glove  \n",
       "0  [-3.90065998e-01  2.85064846e-01  1.03168570e-...  \n",
       "1  [ 5.35968468e-02  2.33869895e-01  2.60694951e-...  \n",
       "2  [-0.09848612 -0.4099722  -0.08234926 -0.131738...  \n",
       "3  [ 0.09402635  0.07983646  0.02374954 -0.276408...  \n",
       "4  [ 2.41189212e-01 -4.62112427e-02  1.89653516e-...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(movie_features_path).drop('Unnamed: 0',axis=1).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79ed7b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>encoded_user_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>3062</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2002-01-16 18:10:54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>4308</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2002-01-16 18:17:06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>4339</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2002-01-16 18:21:26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2002-01-16 18:22:58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>2028</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2002-01-16 18:24:41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating            timestamp  encoded_user_ids\n",
       "0       7     3062     3.0  2002-01-16 18:10:54                 0\n",
       "1       7     4308     4.0  2002-01-16 18:17:06                 0\n",
       "2       7     4339     4.0  2002-01-16 18:21:26                 0\n",
       "3       7       16     3.0  2002-01-16 18:22:58                 0\n",
       "4       7     2028     5.0  2002-01-16 18:24:41                 0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(rating_features_path).drop('Unnamed: 0',axis=1).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1314d3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class User:\n",
    "    def __init__(self, userId, encodedUserId, movieRating):\n",
    "        self.userId = userId\n",
    "        self.encodedUserId = encodedUserId\n",
    "        self.movieRating = movieRating\n",
    " \n",
    "    def __str__(self):\n",
    "        user_info = f\"Encoded UserId: {self.encodedUserId}\\nUserId: {self.userId}\\nMovies Rating: {self.movieRating}\\n\"\n",
    "        return user_info\n",
    "   \n",
    "    def get_user_rating(self, movie_id):\n",
    "        # df = self.movieRating[self.movieRating['movieId']==movie_id]\n",
    "        # if df and len(df)>0:\n",
    "        #     rating = df['rating'].tolist()[0]\n",
    "        # else:\n",
    "        #     return 0.0\n",
    "        for movieid, rating in self.movieRating:\n",
    "            if movie_id == movieid:\n",
    "                return rating\n",
    "        return 0.0\n",
    "       \n",
    " \n",
    "class Movie:\n",
    "    def __init__(self, movieId, encodedMovieId, title, genre, tag, desc, descEmbedding):\n",
    "        self.movieId = movieId\n",
    "        self.encodedMovieId = encodedMovieId\n",
    "        self.title = title\n",
    "        self.genre = genre\n",
    "        self.tag = tag\n",
    "        self.desc = desc\n",
    "        self.descEmbedding = descEmbedding\n",
    "   \n",
    "    def __str__(self):\n",
    "        movie_info = f\"\\nEncoded MovieId: {self.encodedMovieId}\\nMovieId: {self.movieId}\\nTitle: {self.title}\\nGenre: {self.genre}\\nTag: {self.tag}\"\n",
    "        return movie_info\n",
    "   \n",
    " \n",
    "class Recommender():\n",
    " \n",
    "    def __init__(self, movie_features_path,ratings_features_path):\n",
    "        super(Recommender, self).__init__()\n",
    "        self.movies_data,self.users_data,self.ratings_data = self.load_features(movie_features_path,\n",
    "                                                                                ratings_features_path)\n",
    "       \n",
    "        self.movies_map,self.users_map = self.get_list_of_objects(self.movies_data,\n",
    "                                                                  self.users_data)\n",
    "       \n",
    "        self.sequences = self.get_sequences(self.movies_data,self.ratings_data)  \n",
    " \n",
    "        self.state_sequences = self.get_user_sequence_combinations()\n",
    "       \n",
    " \n",
    "    def load_features(self,movie_features_path,ratings_features_path):\n",
    "        movies = pd.read_csv(movie_features_path).drop(['Unnamed: 0'], axis=1)\n",
    "        ratings = pd.read_csv(ratings_features_path).drop(['Unnamed: 0','timestamp'], axis=1)\n",
    " \n",
    "        unique_users = ratings['userId'].unique() # Get unique users\n",
    "       \n",
    "        test_size = len(unique_users)*10//100 # 10% of the users count\n",
    " \n",
    "        train_users = unique_users[: len(unique_users)-test_size] # 90% of the users is used for training\n",
    "        test_users = unique_users[len(unique_users)-test_size:]    # 10% of the users is used for testing\n",
    " \n",
    "        train_ratings = ratings[ratings['userId'].isin(train_users)] # Select from ratings only of train users\n",
    "        test_ratings = ratings[ratings['userId'].isin(test_users)] # Select from ratings only of test users\n",
    "       \n",
    "        print('Number of unique users: ', len(unique_users))\n",
    "        print('Train unique users: ', len(train_users))\n",
    "        print('Test unique users: ', len(test_users))\n",
    "       \n",
    "        print('Total ratings size: ', ratings.shape)\n",
    "        print('Train ratings size: ', train_ratings.shape)\n",
    "        print('Test ratings size: ', test_ratings.shape)\n",
    " \n",
    "        users = train_ratings.groupby(['userId','encoded_user_ids']).apply(lambda x: list(zip(x['movieId'], x['rating']))).reset_index()\n",
    "        users.rename(columns = {0:'movie_rating'}, inplace = True)\n",
    " \n",
    "        return movies, users, train_ratings\n",
    "   \n",
    "    def get_list_of_objects(self,movies_data, user_data):\n",
    "        users_map = {};movies_map = {}\n",
    "       \n",
    " \n",
    "        for row in user_data.itertuples():\n",
    "            user_id = row.userId\n",
    "            encoded_user_id = row.encoded_user_ids\n",
    "            movie_rating = row.movie_rating\n",
    "            user = User(user_id, encoded_user_id, movie_rating)\n",
    "            users_map[user_id]=user\n",
    " \n",
    "        for row in movies_data.itertuples():\n",
    "            movie_id = row.movieId\n",
    "            encoded_movie_id = row.encoded_movie_ids\n",
    "            title = row.title\n",
    "            genre = row.genres\n",
    "            tags = row.tag\n",
    "            desc = row.desc\n",
    "            desc_embedding = np.array(row.desc_embedding_glove.replace('\\n', '').replace('[', '').replace(']', '').split(), dtype=np.float64)\n",
    "            movie = Movie(movie_id, encoded_movie_id, title, genre, tags, desc, desc_embedding)\n",
    "            movies_map[movie_id]=movie\n",
    "       \n",
    "        return movies_map, users_map\n",
    "   \n",
    " \n",
    "    def get_sequences(self,moviesdf, ratingsdf):\n",
    " \n",
    "        # Assuming you have a list of all movie IDs\n",
    "        all_movies = moviesdf.movieId.unique().tolist()\n",
    "       \n",
    "        # Assuming you have encoded_user_ids in ratingsdf\n",
    "        all_users = ratingsdf.userId.unique().tolist()\n",
    " \n",
    "        user_objects = []\n",
    "   \n",
    "        for user_id in all_users:\n",
    "            user_ratings = ratingsdf[ratingsdf['userId'] == user_id]\n",
    "            # Extracting user details\n",
    "            user_details = {\n",
    "                'user_id': user_id,                \n",
    "                'ratings_df': user_ratings\n",
    "            }\n",
    "            user_movies_watched = user_ratings['movieId'].tolist()\n",
    "            user_movie_objects = []\n",
    "            for movie_id in user_movies_watched:\n",
    "                movie_details = moviesdf[moviesdf['movieId'] == movie_id]\n",
    "                # Extracting movie details\n",
    "                movie_object = {\n",
    "                    'movie_id': movie_id,\n",
    "                    'genre': movie_details['genres'].iloc[0],\n",
    "                    'tag': movie_details['tag'].iloc[0],\n",
    "                    'title': movie_details['title'].iloc[0],\n",
    "                    'desc': movie_details['desc'].iloc[0],\n",
    "                    'desc_embedding': movie_details['desc_embedding_glove'].iloc[0]\n",
    "                }\n",
    "                user_movie_objects.append(movie_object)\n",
    "            user_objects.append({\n",
    "                'user_details': user_details,\n",
    "                'movie_objects': user_movie_objects\n",
    "            })\n",
    "       \n",
    "        # Creating sequences\n",
    "        sequence_dict = {}\n",
    "       \n",
    "        for user_obj in user_objects:\n",
    "            user_id = user_obj['user_details']['user_id']\n",
    "            user_ratings = user_obj['user_details']['ratings_df']\n",
    "            movie_seq = user_ratings['movieId'].tolist()\n",
    "            # Take n as initially watched movies\n",
    "            n = 5;\n",
    "            # Take m as number of recommended movies\n",
    "            m = 5\n",
    "            # Neg pos ratio\n",
    "            k = 2\n",
    "            sequence_lst = []\n",
    "            while n + m <= len(movie_seq):\n",
    "                movie_watched_bucket = movie_seq[:n]\n",
    "                movie_to_be_watched_bucket = movie_seq[n:]\n",
    "                common_pos_samples = list(set(movie_watched_bucket).union(set(movie_to_be_watched_bucket)))\n",
    "                total_neg_samples = [mv for mv in all_movies if mv not in movie_seq]\n",
    "                # num_neg_samples = min(k * (n+m), len(total_neg_samples))\n",
    "                # selected_neg_samples = random.sample(total_neg_samples, num_neg_samples)\n",
    "                selected_mix_samples = total_neg_samples + common_pos_samples\n",
    "                random.shuffle(selected_mix_samples)\n",
    "                sequence_lst.append({'pos_samples': movie_watched_bucket,\n",
    "                                     'mix_samples': selected_mix_samples,\n",
    "                                     'next_pos_samples': movie_to_be_watched_bucket})\n",
    "                n = n + m\n",
    "            sequence_dict[user_id] = sequence_lst\n",
    " \n",
    "        # Convert sequence_dict to DataFrame\n",
    "        df = pd.DataFrame(sequence_dict.items(), columns=['user_id', 'sequences'])\n",
    " \n",
    " \n",
    "        return df\n",
    " \n",
    "    def get_user_sequence_combinations(self):\n",
    "       \n",
    "        user_seq_lst = {}\n",
    "        for indx, row in self.sequences.iterrows():    \n",
    "            user_id = row['user_id']\n",
    "            user_ob = self.users_map[user_id]\n",
    "            sequence_lst = row['sequences']\n",
    "            row_lst = []\n",
    "            for seq in sequence_lst:\n",
    "                seq_dict = {}\n",
    "                pos_neg_samples = seq['mix_samples']  \n",
    "                seq_dict['movie_indices'] = np.array([self.movies_map[movie_id].encodedMovieId for movie_id in  pos_neg_samples],dtype=\"float\")\n",
    "                seq_dict['movie_embeddings'] = np.array([self.movies_map[movie_id].descEmbedding for movie_id in pos_neg_samples],dtype=\"float\")\n",
    "                seq_dict['movie_ratings'] =  np.array([user_ob.get_user_rating(movie_id) for movie_id in pos_neg_samples],dtype=\"float\")\n",
    "                seq_dict['user_indices'] = np.array([user_ob.encodedUserId for movie_id in  pos_neg_samples],dtype=\"float\")\n",
    "                row_lst.append(seq_dict)\n",
    "            user_seq_lst[user_ob.encodedUserId] = row_lst\n",
    "       \n",
    "        return user_seq_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa25cbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users:  14867\n",
      "Train unique users:  13381\n",
      "Test unique users:  1486\n",
      "Total ratings size:  (404333, 4)\n",
      "Train ratings size:  (364315, 4)\n",
      "Test ratings size:  (40018, 4)\n"
     ]
    }
   ],
   "source": [
    "recommender = Recommender(movie_features_path,rating_features_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca934199",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "\n",
    "    def __init__(self, user_id, seq_id, rec,total_items):\n",
    "        \n",
    "        self.userIndexes = np.array(rec.state_sequences[user_id][seq_id]['user_indices'])\n",
    "        self.movieIndexes = np.array(rec.state_sequences[user_id][seq_id]['movie_indices'])\n",
    "        self.movieEmbeddings = np.array(rec.state_sequences[user_id][seq_id]['movie_embeddings'])\n",
    "        self.movieRatings = np.array(rec.state_sequences[user_id][seq_id]['movie_ratings'])        \n",
    "\n",
    "    def get_state(self):\n",
    "        observations = { \"user_indices\":self.userIndexes, \"movie_indices\":self.movieIndexes, \"movie_embeddings\":self.movieEmbeddings, \n",
    "                        \"movie_ratings\":self.movieRatings}\n",
    "        return observations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b09b2481",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommendationPolicyCNN(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, user_embedding_dim,\n",
    "                 movie_embedding_dim, mlp_dims, conv_out_channels, kernel_size):\n",
    "        super(RecommendationPolicyCNN, self).__init__()\n",
    "        \n",
    "        self.num_users = num_users\n",
    "        self.num_movies = num_movies\n",
    "        self.user_embedding_dim = user_embedding_dim\n",
    "        self.movie_embedding_dim = movie_embedding_dim\n",
    "        self.conv_out_channels = conv_out_channels\n",
    "\n",
    "        # print(\"RecommendationPolicyCNN Policy Initialized.\") \n",
    "\n",
    "        # User Embeddings\n",
    "        self.user_embedding_gmf = nn.Embedding(num_users, user_embedding_dim)\n",
    "        self.user_embedding_mlp = nn.Embedding(num_users, user_embedding_dim)\n",
    "\n",
    "        # Convolutional layer for movie embeddings\n",
    "        self.conv1d = nn.Conv1d(in_channels=1, out_channels=conv_out_channels, kernel_size=kernel_size, stride=1, padding=(kernel_size - 1) // 2)\n",
    "\n",
    "        # GMF\n",
    "        self.gmf_fc = nn.Linear(conv_out_channels, user_embedding_dim)\n",
    "\n",
    "        # MLP\n",
    "        mlp_input_dim = user_embedding_dim + conv_out_channels  # Concatenated dimension for user and movie embeddings\n",
    "        self.mlp_fc_layers = nn.ModuleList([nn.Linear(mlp_input_dim, mlp_dims[0])])\n",
    "        for in_size, out_size in zip(mlp_dims[:-1], mlp_dims[1:]):\n",
    "            self.mlp_fc_layers.append(nn.Linear(in_size, out_size))\n",
    "\n",
    "        # Final prediction layer outputs a score for each movie\n",
    "        self.output_fc = nn.Linear(user_embedding_dim + mlp_dims[-1], 1)  # Adjusted to output a single score\n",
    "\n",
    "        # print(\"RecommendationPolicyCNN Policy Initialize Completed.\") \n",
    "\n",
    "    def forward(self, user_ids, movie_ids, movie_embeddings, ratings):\n",
    "\n",
    "        # print(\"RecommendationPolicyCNN Policy Forwarded.\")\n",
    "\n",
    "        user_ids = user_ids.long()\n",
    "        \n",
    "        # Get user embeddings\n",
    "        user_embed_gmf = self.user_embedding_gmf(user_ids)[0]  # Shape: [batch_size, user_embedding_dim]\n",
    "        user_embed_mlp = self.user_embedding_mlp(user_ids)\n",
    "\n",
    "        # Process movie embeddings through Conv1D\n",
    "        movie_embeddings = movie_embeddings.unsqueeze(1).float()  # Shape: [batch_size, 1, movie_embedding_dim] \n",
    "        conv_out = F.relu(self.conv1d(movie_embeddings))  # Shape after Conv1D: [batch_size, conv_out_channels, movie_embedding_dim]\n",
    " \n",
    "        # Assuming we want to collapse the last dimension to align with user_embed_gmf for element-wise multiplication\n",
    "        conv_out_flattened = conv_out.mean(dim=2)  # Averaging across the last dimension, Shape: [batch_size, conv_out_channels]\n",
    "        \n",
    "        # GMF path\n",
    "        gmf_out = self.gmf_fc(conv_out_flattened)  # Ensure this operation results in Shape: [batch_size, user_embedding_dim]\n",
    "        gmf_out = gmf_out * user_embed_gmf  # Now this should work as both tensors have a shape of [batch_size, user_embedding_dim]\n",
    " \n",
    "        # MLP path\n",
    "        mlp_input = torch.cat((user_embed_mlp, conv_out_flattened), dim=1)\n",
    "        mlp_out = mlp_input\n",
    "        for layer in self.mlp_fc_layers:\n",
    "            mlp_out = F.relu(layer(mlp_out))\n",
    "    \n",
    "        # Combine GMF and MLP outputs\n",
    "        combined_features = torch.cat((gmf_out, mlp_out), dim=1)\n",
    "        scores = self.output_fc(combined_features).squeeze()\n",
    "\n",
    "        return scores\n",
    "#         # Adjust scores for visited and unvisited movies\n",
    "#         # scores[ratings > 0] = float('-inf')  # Invalidate scores for visited movies        \n",
    "    \n",
    "#         # # Get top 5 unvisited movie IDs based on scores\n",
    "#         # _, top_indices = torch.topk(scores, 5)\n",
    "#         # top_movie_ids = movie_ids[top_indices]\n",
    "\n",
    "#         # # print(\"RecommendationPolicyCNN Policy Forward Complete.\")\n",
    "#         # print('top_movie_ids ',top_movie_ids)\n",
    "#         # return top_movie_ids\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b56b184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRecommendationFeaturesExtractor(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    Feature extractor that adapts the input observation space to the inputs expected by RecommendationPolicyCNN.\n",
    " \n",
    "        num_users = 15000\n",
    "        num_movies = 200\n",
    "        user_embedding_dim = 8\n",
    "        movie_embedding_dim = 100\n",
    "        mlp_dims = [512, 256, 128]\n",
    "        conv_out_channels = 128\n",
    "        kernel_size = 3\n",
    "    \"\"\"\n",
    "    def __init__(self, observation_space: spaces.Dict,\n",
    "                \n",
    "                 num_users=14867, num_movies=184, user_embedding_dim=8, \n",
    "                 movie_embedding_dim=100, mlp_dims=[512,256,128], \n",
    "                 conv_out_channels=128, kernel_size=3):\n",
    "        \n",
    "        super().__init__(observation_space, features_dim=184)  # The features_dim parameter is arbitrary here\n",
    "       \n",
    "        self.recommendation_policy_cnn = RecommendationPolicyCNN(\n",
    "            num_users=num_users,\n",
    "            num_movies=num_movies,\n",
    "            user_embedding_dim=user_embedding_dim,\n",
    "            movie_embedding_dim=movie_embedding_dim,\n",
    "            mlp_dims=mlp_dims,\n",
    "            conv_out_channels=conv_out_channels,\n",
    "            kernel_size=kernel_size\n",
    "        )\n",
    "       \n",
    " \n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "       \n",
    "        user_indices = torch.tensor(observations[\"user_indices\"].squeeze(0),dtype=torch.float32)\n",
    "        movie_embeddings = torch.tensor(observations[\"movie_embeddings\"].squeeze(0),dtype=torch.float32)\n",
    "        movie_ratings = torch.tensor(observations[\"movie_ratings\"].squeeze(0),dtype=torch.float32)\n",
    "        movie_indices = torch.tensor(observations[\"movie_indices\"].squeeze(0),dtype=torch.float32)\n",
    "     \n",
    "        features = self.recommendation_policy_cnn(user_indices, movie_indices,movie_embeddings, movie_ratings)\n",
    "        print('features :: ',features.shape)\n",
    "        return features\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b4e9ab3-be2a-4868-a077-829aa297336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomActorCriticPolicy(ActorCriticPolicy):\n",
    "    \"\"\"\n",
    "    Custom Actor-Critic policy that uses the recommendation model's output as features for action and value prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, observation_space: spaces.Dict, action_space: spaces.Space, lr_schedule, **kwargs):\n",
    "        super(CustomActorCriticPolicy, self).__init__(observation_space, action_space, lr_schedule, \n",
    "                                                      features_extractor_class=CustomRecommendationFeaturesExtractor,                                                     \n",
    "                                                      **kwargs)\n",
    "\n",
    "        # The action_net predicts which movie to recommend (action_space.n options)\n",
    "        print('self.features_extractor.features_dim',self.features_extractor.features_dim , action_space.shape[0])\n",
    "        self.action_net = nn.Linear(self.features_extractor.features_dim, action_space.shape[0])\n",
    "        # The value_net estimates the value of each state\n",
    "        self.value_net = nn.Linear(self.features_extractor.features_dim,1)\n",
    "\n",
    "    def forward(self, obs: torch.Tensor, deterministic: bool = False) -> torch.Tensor:\n",
    "        features = self.extract_features(obs)\n",
    "        #features = features.view(-1, self.features_dim)\n",
    "        action_scores = self.action_net(features)\n",
    "        values = self.value_net(features)\n",
    "\n",
    "        # Assuming action_scores are raw scores from which we need probabilities\n",
    "        action_probs = F.softmax(action_scores, dim=-1)  # Convert scores to probabilities\n",
    "        action_log_probs = F.log_softmax(action_scores, dim=-1)  # For loss calculation\n",
    "\n",
    "        # # Deterministic actions use the max probability action\n",
    "        # deterministic_actions = torch.argmax(action_probs, dim=1)\n",
    "\n",
    "        # # Stochastic actions sample from the probabilities\n",
    "        # dist = torch.distributions.Categorical(probs=action_probs)\n",
    "        # stochastic_actions = dist.sample()\n",
    "\n",
    "        # actions = deterministic_actions if deterministic else stochastic_actions\n",
    "        \n",
    "        print('action_probs ',action_probs)\n",
    "        print('values ',values)\n",
    "        return action_scores, values , action_log_probs\n",
    "\n",
    "    def extract_features(self, obs: torch.Tensor) -> torch.Tensor:\n",
    "        return self.features_extractor(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd8942f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    " \n",
    "class RecommenderEnv(gym.Env):\n",
    "    def __init__(self, recommender):\n",
    "        super(RecommenderEnv, self).__init__()\n",
    "        self.recommender = recommender\n",
    "        self.current_user_index = 0\n",
    "        self.current_sequence_index = 0\n",
    "        self.num_reco = 5\n",
    " \n",
    "        self.movie_embedding_dim = 100\n",
    "        self.batch_size = len(self.recommender.state_sequences[self.current_user_index][self.current_sequence_index]['movie_indices'])       \n",
    "        self.render_mode = 'human'\n",
    "\n",
    "        self.max_item_len=len(self.recommender.movies_data.movieId.unique().tolist()) \n",
    "        print(' max item len is ',self.max_item_len)\n",
    "        # Example of action and observation space definitions\n",
    "        # These should be tailored to your specific requirements\n",
    "        #self.action_space = spaces.MultiDiscrete(self.num_reco) # Assuming actions are selecting among movies\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(self.max_item_len,), dtype=np.float32)\n",
    "        \n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"user_indices\": spaces.Box(low=0, high=len(recommender.users_map), shape=(self.max_item_len,), dtype=float),           \n",
    "            \"movie_indices\": spaces.Box(low=0, high=len(recommender.movies_map), shape=(self.max_item_len,), dtype=float),           \n",
    "            \"movie_embeddings\": spaces.Box(low=-float('inf'), high=float('inf'), shape=(self.max_item_len, self.movie_embedding_dim), dtype=float),\n",
    "            \"movie_ratings\": spaces.Box(low=0.0, high=5.0, shape=(self.max_item_len,), dtype=float)\n",
    "           \n",
    "        })\n",
    " \n",
    "        # print(\"RecommenderEnv Initizalized.\")\n",
    " \n",
    " \n",
    "    def step(self, action):\n",
    "       \n",
    "        print('Action is ', action) \n",
    "        done = False \n",
    "        # Logic to iterate through users and their sequences\n",
    "        if self.current_sequence_index < len(self.recommender.sequences) - 1:\n",
    "            self.current_sequence_index += 1\n",
    "        else:\n",
    "            self.current_user_index += 1\n",
    "            self.current_sequence_index = 0\n",
    "       \n",
    "        if self.current_user_index >= len(self.recommender.users_map):\n",
    "            done = True # End of episode       \n",
    "        # print('Actual Next Value' , actual_next_value) \n",
    "        # Placeholder for reward calculation and state update\n",
    "        reward = 0       \n",
    "        print('Reward' , reward) \n",
    "        state = self._get_current_state() \n",
    "\n",
    "        print('State ' ,state)\n",
    "\n",
    "        \n",
    "        return state, reward, done, {}\n",
    " \n",
    "    def reset(self):\n",
    "        # Reset the environment state to the beginning\n",
    "        self.current_user_index = 0\n",
    "        self.current_sequence_index = 0\n",
    "        return self._get_current_state()\n",
    "   \n",
    "    def _get_current_state(self):\n",
    "        # Implement logic to return the current state based on the current user and sequence index\n",
    "        # This is where you'd extract the current user's pos_neg_samples, ratings, and movie_embeddings       \n",
    "        # state = State(self.recommender.sequences['user_id'][self.current_user_index], self.recommender.sequences['sequences'][self.current_user_index][self.current_sequence_index], self.recommender)\n",
    "        state = State(self.current_user_index, self.current_sequence_index, self.recommender,self.max_item_len)\n",
    "        return state.get_state()\n",
    " \n",
    "    def render(self, mode='human'):\n",
    "        # Optional for visualization\n",
    "        pass\n",
    " \n",
    "    def close(self):\n",
    "        # Optional cleanup\n",
    "        pass\n",
    " \n",
    "    def seed(self, seed=None):\n",
    "        np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfce7cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    " \n",
    "def make_recommender_env(num_env):\n",
    "    \"\"\"\n",
    "    Utility function for creating the vectorized environment.\n",
    " \n",
    "    :param num_env: Number of parallel environments to create.\n",
    "    :return: A vectorized Gym environment.\n",
    "    \"\"\"\n",
    "    def _init():\n",
    "        # print(\"Env Creation Initizalized.\")\n",
    "        return RecommenderEnv(recommender)\n",
    "    # Note: Removed the `env_id` parameter from the call\n",
    "    return make_vec_env(_init, n_envs=num_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ae20d0f-9944-4ef3-a724-43ad7c6ddb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " max item len is  184\n"
     ]
    }
   ],
   "source": [
    "# Create the environment and initialize the model\n",
    "env = make_recommender_env(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f653ce7b-e4a6-45c6-8dd5-e16534c35922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-1.0, 1.0, (184,), float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0237fd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " max item len is  184\n",
      "Using cpu device\n",
      "self.features_extractor.features_dim 184 184\n",
      "features ::  torch.Size([184])\n",
      "action_probs  tensor([0.0049, 0.0048, 0.0054, 0.0050, 0.0061, 0.0049, 0.0058, 0.0054, 0.0053,\n",
      "        0.0057, 0.0052, 0.0054, 0.0057, 0.0067, 0.0053, 0.0046, 0.0053, 0.0054,\n",
      "        0.0053, 0.0049, 0.0058, 0.0053, 0.0057, 0.0048, 0.0051, 0.0048, 0.0054,\n",
      "        0.0054, 0.0052, 0.0057, 0.0053, 0.0061, 0.0052, 0.0055, 0.0055, 0.0056,\n",
      "        0.0055, 0.0056, 0.0057, 0.0053, 0.0049, 0.0056, 0.0053, 0.0056, 0.0065,\n",
      "        0.0055, 0.0051, 0.0058, 0.0054, 0.0057, 0.0055, 0.0054, 0.0053, 0.0058,\n",
      "        0.0050, 0.0057, 0.0055, 0.0055, 0.0051, 0.0059, 0.0055, 0.0052, 0.0051,\n",
      "        0.0061, 0.0049, 0.0061, 0.0053, 0.0051, 0.0050, 0.0054, 0.0062, 0.0049,\n",
      "        0.0049, 0.0049, 0.0053, 0.0053, 0.0062, 0.0058, 0.0057, 0.0059, 0.0065,\n",
      "        0.0056, 0.0047, 0.0057, 0.0058, 0.0054, 0.0063, 0.0053, 0.0054, 0.0056,\n",
      "        0.0055, 0.0052, 0.0055, 0.0056, 0.0051, 0.0052, 0.0063, 0.0053, 0.0051,\n",
      "        0.0057, 0.0054, 0.0047, 0.0058, 0.0051, 0.0067, 0.0056, 0.0054, 0.0055,\n",
      "        0.0051, 0.0046, 0.0053, 0.0052, 0.0049, 0.0053, 0.0051, 0.0053, 0.0065,\n",
      "        0.0060, 0.0053, 0.0057, 0.0046, 0.0055, 0.0049, 0.0061, 0.0059, 0.0055,\n",
      "        0.0061, 0.0056, 0.0055, 0.0058, 0.0057, 0.0051, 0.0055, 0.0053, 0.0052,\n",
      "        0.0054, 0.0056, 0.0055, 0.0057, 0.0051, 0.0061, 0.0050, 0.0051, 0.0062,\n",
      "        0.0055, 0.0047, 0.0057, 0.0052, 0.0051, 0.0055, 0.0053, 0.0050, 0.0051,\n",
      "        0.0047, 0.0056, 0.0053, 0.0054, 0.0052, 0.0050, 0.0063, 0.0053, 0.0058,\n",
      "        0.0054, 0.0050, 0.0049, 0.0051, 0.0056, 0.0056, 0.0057, 0.0048, 0.0059,\n",
      "        0.0048, 0.0056, 0.0057, 0.0053, 0.0061, 0.0053, 0.0052, 0.0050, 0.0057,\n",
      "        0.0060, 0.0050, 0.0051, 0.0059])\n",
      "values  tensor([-0.0876])\n",
      "Action is  -0.09544799\n",
      "Reward 0\n",
      "State  {'user_indices': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'movie_indices': array([ 34.,   5., 145.,  32.,  36., 169., 129.,  80.,  57.,  74.,  42.,\n",
      "        94., 153., 110.,  69.,  88.,  25., 154.,  49., 116., 134.,  18.,\n",
      "       126.,  54.,  97., 168.,  87.,  51.,  21., 144., 157.,  71.,  38.,\n",
      "       121., 125., 161.,  82., 133.,  99.,  67., 123.,  59.,  43.,  90.,\n",
      "        68.,   6., 173., 142.,  75.,  29., 109.,  41., 122., 128., 151.,\n",
      "        79.,  35., 166.,  11.,  30.,  89., 112., 143., 160., 135.,  48.,\n",
      "       163.,  58.,  50., 113., 108.,  52., 150., 127.,  81.,  27.,  44.,\n",
      "         4.,  93., 179., 146.,  62.,  46., 181.,  14., 141., 156., 102.,\n",
      "       180.,  78.,  66., 138.,  73., 132.,  98., 106.,  72., 172.,  65.,\n",
      "         3., 176., 158., 124.,  70.,  96.,  39., 164.,  12., 177., 136.,\n",
      "        86.,  53.,  28., 105.,   2., 119., 148., 120.,  26., 175.,  20.,\n",
      "        22., 118., 107.,  56., 101., 174.,  63., 114., 152., 103.,  31.,\n",
      "       149.,   1.,  60., 111.,   8., 167.,  64.,  47., 140.,  92.,  24.,\n",
      "         9.,  85., 137., 162.,  61.,  91., 159., 178., 165.,  15.,  45.,\n",
      "        83., 130.,  23., 182.,  33.,  84.,  40., 131.,  19.,  16.,  37.,\n",
      "        55.,   0.,  17., 147., 115.,  76., 139., 100.,  77., 170.,  10.,\n",
      "       183., 155., 171., 117., 104.,   7.,  13.,  95.]), 'movie_embeddings': array([[-0.3381978 ,  0.22170687,  0.5049904 , ..., -0.16482013,\n",
      "         0.14908569, -0.30516216],\n",
      "       [ 0.04718307,  0.10339734,  0.09114828, ..., -0.39886737,\n",
      "         0.23571408,  0.26172417],\n",
      "       [-0.0083412 ,  0.6503486 ,  0.31538776, ..., -0.12534875,\n",
      "         0.1262148 ,  0.16884567],\n",
      "       ...,\n",
      "       [ 0.14529552,  0.25462282,  0.12069529, ..., -0.28815913,\n",
      "        -0.05052598,  0.16534778],\n",
      "       [-0.2117725 ,  0.30992499,  0.37977126, ..., -0.22207624,\n",
      "         0.0814797 , -0.08252907],\n",
      "       [-0.17842682,  0.19267876,  0.32582933, ..., -0.13189836,\n",
      "         0.10452514, -0.36032784]]), 'movie_ratings': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 3., 5., 0., 0., 0.,\n",
      "       0., 5., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 3., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 4., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       4., 0., 0., 0., 4., 0., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 3., 3., 0., 3., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 3., 0., 0., 4., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 5., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (184,) into shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(CustomActorCriticPolicy, env, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# model.save('ppo_recommender')\u001b[39;00m\n",
      "File \u001b[1;32mC:\\softwares\\python-3.11.4\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\softwares\\python-3.11.4\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:277\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 277\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\softwares\\python-3.11.4\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:223\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    220\u001b[0m             terminal_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mpredict_values(terminal_obs)[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    221\u001b[0m         rewards[idx] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m terminal_value\n\u001b[1;32m--> 223\u001b[0m \u001b[43mrollout_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_last_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_last_episode_starts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;241m=\u001b[39m new_obs  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_episode_starts \u001b[38;5;241m=\u001b[39m dones\n",
      "File \u001b[1;32mC:\\softwares\\python-3.11.4\\Lib\\site-packages\\stable_baselines3\\common\\buffers.py:796\u001b[0m, in \u001b[0;36mDictRolloutBuffer.add\u001b[1;34m(self, obs, action, reward, episode_start, value, log_prob)\u001b[0m\n\u001b[0;32m    794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_starts[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(episode_start)\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m--> 796\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_probs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m log_prob\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    797\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size:\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (184,) into shape (1,)"
     ]
    }
   ],
   "source": [
    "# Create the environment and initialize the model\n",
    "env = make_recommender_env(1) \n",
    "#model = PPO(CustomActorCriticPolicy, env, verbose=1)\n",
    "model = PPO(CustomActorCriticPolicy, env, learning_rate=1e-4, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=100)\n",
    "\n",
    "# Save the model\n",
    "# model.save('ppo_recommender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee00d6f-b49a-4bd1-94ad-932ae1b87a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991fb094-328e-463a-bd2f-beef751ba192",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
